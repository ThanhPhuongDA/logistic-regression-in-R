# OPEN the training data set from the working directory (data downloaded from Kaggle website)
train <- read.csv('titanic wtrain.csv')
# READ some front rows of the data set
head(train)
  PassengerId Survived Pclass                                                Name    Sex
1           1        0      3                             Braund, Mr. Owen Harris   male
2           2        1      1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female
3           3        1      3                              Heikkinen, Miss. Laina female
4           4        1      1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female
5           5        0      3                            Allen, Mr. William Henry   male
6           6        0      3                                    Moran, Mr. James   male
  Age SibSp Parch           Ticket    Fare Cabin Embarked
1  22     1     0        A/5 21171  7.2500              S
2  38     1     0         PC 17599 71.2833   C85        C
3  26     0     0 STON/O2. 3101282  7.9250              S
4  35     1     0           113803 53.1000  C123        S
5  35     0     0           373450  8.0500              S
6  NA     0     0           330877  8.4583              Q

# CHECK the DATA SET if there any missing data
# Check if there are any missing data. 
any(is.na(train))
[1] TRUE

Then I use the Amelia pacakge to see which explanatory variables are missing
install.packages('Amelia')
library(Amelia)

Loading required package: Rcpp
## 
## Amelia II: Multiple Imputation
## (Version 1.7.6, built: 2019-11-24)
## Copyright (C) 2005-2021 James Honaker, Gary King and Matthew Blackwell
## Refer to http://gking.harvard.edu/amelia/ for more information
## 

 missmap(train, main =' missing map', col = c('yellow', 'black'), legend = FALSE)
 
 As can be seen from the graph, data in the Age variables are missing
 
 # EXPLORE THE DATA FURTHER
 
library(ggplot2)
library(ggthemes)
ggplot(train, aes(Survived)) + geom_bar()
##The probability of survival: > 0.5, the probability of Deceased: <0.5
ggplot(train, aes(Sex)) + geom_bar(aes(fill = factor(Sex)), alpha = 0.5)
##The number of male passengers doubled the number of female ones.
ggplot(train, aes(Pclass)) + geom_bar(aes(fill = factor(Pclass)), alpha = 0.5)
## Class 3 ranked the highest, twice as high as the class 1 and class 2 was the lowest
ggplot(train, aes(SibSp)) + geom_bar(fill = 'green', alpha = 0.5)
## Most of passengers did not bring siblings or spouses on board.
ggplot(train, aes(Parch)) + geom_bar(fill = 'red', alpha = 0.5)
## Most of the passengers did not travel with their parents or children
ggplot(train,aes(Fare)) + geom_histogram(bins = 20, fill='blue',color='black', alpha = 0.5)
## Most passengers bought cheaper fares less than 200.

# CLEAN THE DATA
#Because Age data was missing, I need to fill in missing age data instead of just removing all data in the column Age. 
#One way to do this is by filling in the mean age of all the passengers (imputation). However we can be smarter about this and check the average age by passenger class.

pl <- ggplot(train,aes(Pclass,Age)) + geom_boxplot(aes(group=Pclass,fill=factor(Pclass),alpha=0.4)) 
pl + scale_y_continuous(breaks = seq(min(0), max(80), by = 2))

Warning message:
Removed 177 rows containing non-finite values (stat_boxplot). 
# As can be seen from the boxplot, passengers in the higher classes(the rich) tend to be older. I then use these average age values to impute based on Pclass for Age.

impute_age <- function(age,class){
    out <- age
    for (i in 1:length(age)){
        
        if (is.na(age[i])){

            if (class[i] == 1){
                out[i] <- 37

            }else if (class[i] == 2){
                out[i] <- 29

            }else{
                out[i] <- 24
            }
        }else{
            out[i]<-age[i]
        }
    }
    return(out)
}

fixed.ages <- impute_age(train$Age, train$Pclass)
train$Age <- fixed.ages
## Run missmap to check if all missing Age data has been filled
missmap(train, main="Missings Map", col=c("yellow", "black"), legend=FALSE)

## It worked!!!, no more missing Age data

# BUILD THE LOGISTIC MODEL

str(train)
'data.frame':	891 obs. of  12 variables:
 $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
 $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Name       : chr  "Braund, Mr. Owen Harris" "Cumings, Mrs. John Bradley (Florence Briggs Thayer)" "Heikkinen, Miss. Laina" "Futrelle, Mrs. Jacques Heath (Lily May Peel)" ...
 $ Sex        : chr  "male" "female" "female" "female" ...
 $ Age        : num  22 38 26 35 35 24 54 2 27 14 ...
 $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Ticket     : chr  "A/5 21171" "PC 17599" "STON/O2. 3101282" "113803" ...
 $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Cabin      : chr  "" "C85" "" "C123" ...
 $ Embarked   : chr  "S" "C" "S" "S" ...
 
 summary(train)
  PassengerId       Survived          Pclass          Name               Sex           
 Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891         Length:891        
 1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character   Class :character  
 Median :446.0   Median :0.0000   Median :3.000   Mode  :character   Mode  :character  
 Mean   :446.0   Mean   :0.3838   Mean   :2.309                                        
 3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                                        
 Max.   :891.0   Max.   :1.0000   Max.   :3.000                                        
      Age            SibSp           Parch           Ticket               Fare       
 Min.   : 0.42   Min.   :0.000   Min.   :0.0000   Length:891         Min.   :  0.00  
 1st Qu.:22.00   1st Qu.:0.000   1st Qu.:0.0000   Class :character   1st Qu.:  7.91  
 Median :26.00   Median :0.000   Median :0.0000   Mode  :character   Median : 14.45  
 Mean   :29.07   Mean   :0.523   Mean   :0.3816                      Mean   : 32.20  
 3rd Qu.:37.00   3rd Qu.:1.000   3rd Qu.:0.0000                      3rd Qu.: 31.00  
 Max.   :80.00   Max.   :8.000   Max.   :6.0000                      Max.   :512.33  
    Cabin             Embarked        
 Length:891         Length:891        
 Class :character   Class :character  
 Mode  :character   Mode  :character  
                                      
 ## Select the relevant columns for training:
library(dplyr)
train <- select(train,-PassengerId,-Name,-Ticket,-Cabin)

head(train,3)
  Survived Pclass    Sex Age SibSp Parch    Fare Embarked
1        0      3   male  22     1     0  7.2500        S
2        1      1 female  38     1     0 71.2833        C
3        1      3 female  26     0     0  7.9250        S
> 

str(train)
'data.frame':	891 obs. of  8 variables:
 $ Survived: int  0 1 1 1 0 0 0 0 1 1 ...
 $ Pclass  : int  3 1 3 1 3 3 1 3 3 2 ...
 $ Sex     : chr  "male" "female" "female" "female" ...
 $ Age     : num  22 38 26 35 35 24 54 2 27 14 ...
 $ SibSp   : int  1 1 0 1 0 0 0 3 0 1 ...
 $ Parch   : int  0 0 0 0 0 0 0 1 2 0 ...
 $ Fare    : num  7.25 71.28 7.92 53.1 8.05 ...
 $ Embarked: chr  "S" "C" "S" "S" ...
 
 ## Group some variables into factor
 train$Survived <- factor(train$Survived)
train$Pclass <- factor(train$Pclass)
train$Parch <- factor(train$Parch)
train$SibSp <- factor(train$SibSp)

# TRAIN THE MODEL

log.model <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = train)

summary(log.model)

Call:
glm(formula = Survived ~ ., family = binomial(link = "logit"), 
    data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.8158  -0.6134  -0.4138   0.5808   2.4896  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.845e+01  1.660e+03   0.011 0.991134    
Pclass2     -1.079e+00  3.092e-01  -3.490 0.000484 ***
Pclass3     -2.191e+00  3.161e-01  -6.930 4.20e-12 ***
Sexmale     -2.677e+00  2.040e-01 -13.123  < 2e-16 ***
Age         -3.971e-02  8.758e-03  -4.534 5.79e-06 ***
SibSp1       8.135e-02  2.245e-01   0.362 0.717133    
SibSp2      -2.897e-01  5.368e-01  -0.540 0.589361    
SibSp3      -2.241e+00  7.202e-01  -3.111 0.001862 ** 
SibSp4      -1.675e+00  7.620e-01  -2.198 0.027954 *  
SibSp5      -1.595e+01  9.588e+02  -0.017 0.986731    
SibSp8      -1.607e+01  7.578e+02  -0.021 0.983077    
Parch1       3.741e-01  2.895e-01   1.292 0.196213    
Parch2       3.862e-02  3.824e-01   0.101 0.919560    
Parch3       3.655e-01  1.056e+00   0.346 0.729318    
Parch4      -1.586e+01  1.055e+03  -0.015 0.988007    
Parch5      -1.152e+00  1.172e+00  -0.983 0.325771    
Parch6      -1.643e+01  2.400e+03  -0.007 0.994536    
Fare         2.109e-03  2.490e-03   0.847 0.397036    
EmbarkedC   -1.458e+01  1.660e+03  -0.009 0.992995    
EmbarkedQ   -1.456e+01  1.660e+03  -0.009 0.993001    
EmbarkedS   -1.486e+01  1.660e+03  -0.009 0.992857    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1186.66  on 890  degrees of freedom
Residual deviance:  763.41  on 870  degrees of freedom
AIC: 805.41

Number of Fisher Scoring iterations: 15
As it can be seen clearly from the result that Sex,Age, and Pclass are the most significant features with three stars. Which makes sense given the women and children first policy.

# PREDICT USING THE TEST CASE
library(caTools)
set.seed(101)
split <- sample.split(train$Survived,SplitRatio = 0.7)
final.train = subset(train, split == TRUE)
final.test = subset(train, split == FALSE)

final.log.model <- glm(formula=Survived ~ . , family = binomial(link='logit'),data = final.train)

summary(final.log.model)

Call:
glm(formula = Survived ~ ., family = binomial(link = "logit"), 
    data = final.train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.8288  -0.5607  -0.4096   0.6174   2.4898  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.777e+01  2.400e+03   0.007 0.994091    
Pclass2     -1.230e+00  3.814e-01  -3.225 0.001261 ** 
Pclass3     -2.160e+00  3.841e-01  -5.624 1.87e-08 ***
Sexmale     -2.660e+00  2.467e-01 -10.782  < 2e-16 ***
Age         -3.831e-02  1.034e-02  -3.705 0.000212 ***
SibSp1      -2.114e-02  2.755e-01  -0.077 0.938836    
SibSp2      -4.000e-01  6.463e-01  -0.619 0.536028    
SibSp3      -2.324e+00  8.994e-01  -2.584 0.009765 ** 
SibSp4      -1.196e+00  8.302e-01  -1.440 0.149839    
SibSp5      -1.603e+01  9.592e+02  -0.017 0.986666    
SibSp8      -1.633e+01  1.004e+03  -0.016 0.987019    
Parch1       7.290e-01  3.545e-01   2.056 0.039771 *  
Parch2       1.406e-01  4.504e-01   0.312 0.754892    
Parch3       7.919e-01  1.229e+00   0.645 0.519226    
Parch4      -1.498e+01  1.552e+03  -0.010 0.992300    
Parch5      -9.772e-03  1.378e+00  -0.007 0.994343    
Parch6      -1.635e+01  2.400e+03  -0.007 0.994563    
Fare         3.128e-03  3.091e-03   1.012 0.311605    
EmbarkedC   -1.398e+01  2.400e+03  -0.006 0.995353    
EmbarkedQ   -1.387e+01  2.400e+03  -0.006 0.995386    
EmbarkedS   -1.431e+01  2.400e+03  -0.006 0.995243    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 829.60  on 622  degrees of freedom
Residual deviance: 530.63  on 602  degrees of freedom
AIC: 572.63

Number of Fisher Scoring iterations: 15

fitted.probabilities <- predict(final.log.model,newdata=final.test,type='response')

fitted.results <- ifelse(fitted.probabilities > 0.5,1,0)
# Calculate the accuracy of the model
misClasificError <- mean(fitted.results != final.test$Survived)
print(paste('Accuracy',1-misClasificError))

[1] "Accuracy 0.798507462686567"
It seems that the model can predict results up to around 80% accuracy.
table(final.test$Survived, fitted.probabilities > 0.5)
   
    FALSE TRUE
  0   140   25
  1    29   74
